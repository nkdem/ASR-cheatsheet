% ==================================================
% == Section 1: ASR Fundamentals & HMM Basics ==
% ==================================================

\wde{Objective of ASR}{
    Given a sequence of acoustic feature vectors $X$,
    and $W$ denotes a word sequence, ASR aims to find the most likely word sequence $W^*$
    \begin{equation*}
        W^* = \argmax_{W} P(W | X)
    \end{equation*}
}

\wc{Decomposition of $P(W | X)$}{
    We can decompose $P(W | X)$ with Bayes' theorem:
    \begin{equation*}
        P(W | X) = \frac{P(X | W) P(W)}{P(X)} \propto P(X | W) P(W)
    \end{equation*}
    $P(X | W)$ is the \textbf{Acoustic Model} (AM) and $P(W)$ is the \textbf{Language Model} (LM).
}

\wde{Modelling the Acoustic Model with HMMs}{
    The AM $P(X|W)$ is typically modelled using Hidden Markov Models (HMMs).
    Words are represented as sequences of phonemes. Each phoneme is often modelled by a left-to-right HMM, typically with 3 emitting states (enforcing a minimum phone duration).
    Word HMMs are formed by concatenating the corresponding phoneme HMMs.
    % TODO: maybe add a diagram here (e.g., from asr07 slide 4 or 7)
}

\wde{The Three Fundamental Problems of HMMs}{
    Given an HMM $\mathcal{M}$ and an observation sequence $X = x_1, \dots, x_T$:
    \begin{enumerate}
        \item \textbf{Likelihood Problem:} Compute the total likelihood of the observation sequence: $P(X|\mathcal{M})$. (Solved by Forward Algorithm).
        \item \textbf{Decoding Problem:} Find the most likely sequence of hidden states $Q^* = q_1, \dots, q_T$. (Solved by Viterbi Algorithm).
        \item \textbf{Training Problem:} Adjust the HMM parameters $\lambda = \{A, B\}$ to maximize $P(X|\mathcal{M})$. (Solved by Baum-Welch/EM Algorithm or Viterbi Training).
    \end{enumerate}
}

% ==================================================
% == Section 2: Core HMM Algorithms ==
% ==================================================

\wa{Forward Algorithm ($\alpha_j(t)$)}{
    Solves the \textbf{Likelihood} problem and is a key part of the \textbf{Training} problem (E-step).
    The forward probability $\alpha_j(t) = P(x_1, \dots, x_t, q_t = j | \mathcal{M})$ is the probability of observing the first $t$ observations and being in state $j$ at time $t$.
    \begin{itemize}
        \item Initialisation (using non-emitting start state $q_0$):
        \begin{align*}
            \alpha_0(0) &= 1 \\
            \alpha_j(0) &= 0 \quad \text{for } j = 1, \dots, J
        \end{align*}
        \item Recursion ($t = 1, \dots, T$; $j = 1, \dots, J$):
        \begin{equation*}
            \alpha_j(t) = \left[ \sum_{i=0}^{J} \alpha_i(t-1) a_{ij} \right] b_j(x_t)
        \end{equation*}
        \item Termination (using non-emitting end state $q_E$):
        \begin{equation*}
            P(X | \mathcal{M}) = \alpha_{E}(T+1) = \sum_{i=1}^{J} \alpha_i(T) a_{iE}
        \end{equation*}
    \end{itemize}
}

\wa{Viterbi Algorithm ($V_j(t), B_j(t)$)}{
    Solves the \textbf{Decoding} problem by finding the single most probable state sequence $Q^*$.
    $V_j(t)$ is the probability of the most probable path ending in state $j$ at time $t$.
    $B_j(t)$ stores the previous state index on that most probable path.
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            V_0(0) &= 1\\
            V_j(0) &= 0 \quad \text{for } j = 1, \dots, J\\
            B_j(0) &= 0 \quad \forall j
        \end{align*}
        \item Recursion ($t = 1, \dots, T$; $j = 1, \dots, J$):
        \begin{align*}
            V_j(t) &= \left[ \max_{i=1 \dots J} V_i(t-1) a_{ij} \right] b_j(x_t) \\ % Adjusted max range assuming emitting states 1..J
            B_j(t) &= \argmax_{i=1 \dots J} V_i(t-1) a_{ij}
            % Or use i=0..J if including transitions from start state directly
        \end{align*}
        \item Termination:
        \begin{align*}
            P(X, Q^* | \mathcal{M}) = V_E &= \max_{i=1 \dots J} V_i(T) a_{iE} \\
            q_T^* = B_E &= \argmax_{i=1 \dots J} V_i(T) a_{iE}
        \end{align*}
        \item Backtracking: $q_{t}^* = B_{q_{t+1}^*}(t+1)$ for $t = T-1, \dots, 1$.
    \end{itemize}
}

\wde{Backward Algorithm ($\beta_j(t)$)}{
    Needed for the E-step of Baum-Welch/EM \textbf{Training}.
    The backward probability $\beta_j(t) = P(x_{t+1}, \dots, x_T | q_t = j, \mathcal{M})$ is the probability of observing the future observation sequence, given state $j$ at time $t$.
    \begin{itemize}
        \item Initialisation ($j = 1, \dots, J$):
        \begin{equation*}
            \beta_j(T) = a_{jE} \quad \text{(Prob. of transitioning to end state E)}
        \end{equation*}
        \item Recursion ($t = T-1, \dots, 1$; $i = 1, \dots, J$):
        \begin{equation*}
            \beta_i(t) = \sum_{j=1}^{J} a_{ij} b_j(x_{t+1}) \beta_j(t+1)
        \end{equation*}
        % Optional: Termination check
        % \item Termination Check: $P(X | \mathcal{M}) = \sum_{j=1}^{J} a_{0j} b_j(x_1) \beta_j(1)$
    \end{itemize}
}

% ==================================================
% == Section 3: HMM Training ==
% ==================================================

\wa{Training Method 1: Viterbi Training (Hard Assignment)}{
    A simpler approach to the \textbf{Training} problem. Uses the Viterbi algorithm to find the single best state alignment, then performs Maximum Likelihood Estimation based on that alignment.
    \begin{enumerate}
        \item \textbf{Alignment:} Run Viterbi with current parameters $\lambda^{\text{old}}$ to get $Q^*$. Define $z_{jt} = 1$ if $q_t^* = j$, else $0$.
        \item \textbf{Re-estimation:} Calculate counts and update parameters $\lambda^{\text{new}}$.
            \begin{align*}
                \hat{a}_{ij}^{\text{new}} &= \frac{\text{Count}(q_t^*=i, q_{t+1}^*=j)}{\text{Count}(q_t^*=i)} \\
                % Assuming single Gaussian output b_j for simplicity here
                \hat{\boldsymbol{\mu}}_j^{\text{new}} &= \frac{\sum_{t: q_t^*=j} \mathbf{x}_t}{\text{Count}(q_t^*=j)} \\
                \hat{\boldsymbol{\Sigma}}_j^{\text{new}} &= \frac{\sum_{t: q_t^*=j} (\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})(\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})^T}{\text{Count}(q_t^*=j)}
            \end{align*}
        \item Repeat until convergence.
    \end{enumerate}
    Note: Optimizes likelihood of the single best path, not the total likelihood $P(X|\mathcal{M})$. Often less accurate than EM.
}

\wa{Training Method 2: Baum-Welch Algorithm (EM)}{
    Solves the \textbf{Training} problem by maximizing the total likelihood $P(X|\mathcal{M})$ using Expectation-Maximization. It uses 'soft' assignments based on probabilities. Requires derived probabilities from the E-step.
}

\wde{E-Step Probabilities for Baum-Welch}{
    Calculated using $\alpha_j(t)$ and $\beta_j(t)$ from the Forward/Backward algorithms run with current parameters $\lambda^{\text{old}}$.
    \begin{itemize}
        \item \textbf{State Occupation Probability ($\gamma_j(t)$):} Prob. of being in state $j$ at time $t$, given $X$.
            \[ \gamma_j(t) = P(q_t = j | X, \mathcal{M}^{\text{old}}) = \frac{\alpha_j(t) \beta_j(t)}{P(X|\mathcal{M}^{\text{old}})} = \frac{\alpha_j(t) \beta_j(t)}{\alpha_E} \]
        \item \textbf{Transition Occupation Probability ($\xi_{ij}(t)$):} Prob. of transition $i \to j$ at time $t \to t+1$, given $X$.
            
        \begin{align*}
        \xi_{ij}(t) = P(q_t = i, q_{t+1} &= j | X, \mathcal{M}^{\text{old}}) \\
        &= \frac{\alpha_i(t) a_{ij}^{\text{old}} b_j^{\text{old}}(x_{t+1}) \beta_j(t+1)}{\alpha_E}
        \end{align*}

    \end{itemize}
}

\wa{Baum-Welch Algorithm Steps}{
    Iteratively perform:
    \begin{itemize}
        \item \textbf{E-step:} Using $\lambda^{\text{old}}$, run Forward and Backward algorithms. Compute expected values: $\gamma_j(t)$ and $\xi_{ij}(t)$ for all $i, j, t$. If using GMMs, also compute $\gamma_{jm}(t)$ (see GMM section).
        \item \textbf{M-step:} Update parameters $\lambda^{\text{new}}$ using the expectations from the E-step to maximize expected complete-data log-likelihood.
            \begin{itemize}
                \item \textbf{Update $a_{ij}$:}
                    \[ \hat a_{ij}^{\text{new}} = \frac{\sum_{t=1}^{T-1} \xi_{ij}(t)}{\sum_{t=1}^{T-1} \gamma_i(t)} \]
                    (Sum numerator/denominator over all utterances $r$ in a corpus).
                \item \textbf{Update $b_j$ (Observation Probabilities):} See details below depending on model (Single Gaussian or GMM).
                % \item \textbf{Update Initial Probabilities $\pi_i = a_{0i}$ (Optional):} $\hat{\pi}_i^{\text{new}} = \gamma_i(1)$
            \end{itemize}
        \item Repeat until convergence of parameters or likelihood $P(X|\lambda)$.
    \end{itemize}
}

% ==================================================
% == Section 4: Observation Probability Models (GMMs) ==
% ==================================================

\wde{M-Step: Observation Probabilities ($b_j(\mathbf{x})$) [Single Gaussian]}{
    If $b_j(\mathbf{x}) = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)$, update using $\gamma_j(t)$ as weights:
    \begin{align*}
        \hat{\boldsymbol{\mu}}_j^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_j(t) \mathbf{x}_t}{\sum_{t=1}^{T} \gamma_j(t)} \\
        \hat{\boldsymbol{\Sigma}}_j^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_j(t) (\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})(\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})^T}{\sum_{t=1}^{T} \gamma_j(t)}
    \end{align*}
}

\wde{Observation Probabilities with GMMs}{
    A single Gaussian per state is often insufficient. Gaussian Mixture Models (GMMs) provide a more powerful density estimate for $b_j(\mathbf{x})$:
    \[ b_j(\mathbf{x}) = p(\mathbf{x} | q = j) = \sum_{m=1}^{M} c_{jm} \mathcal{N}(\mathbf{x} ; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm}) \]
    Training requires estimating parameters for each component $m$ within each state $j$.
}

\wde{Component-State Occupation Probability ($\gamma_{jm}(t)$)}{
    Needed for training GMMs within HMMs. Calculated in the E-step. It's the probability of being in state $j$ and using component $m$ at time $t$, given $X$.
    \begin{align*}
        \gamma_{jm}(t) &= P(q_t = j, c_t = m | X, \mathcal{M}^{\text{old}}) \\
        &= \gamma_j(t) \times \frac{c_{jm}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jm}^{\text{old}}, \boldsymbol{\Sigma}_{jm}^{\text{old}})}{b_j^{\text{old}}(\mathbf{x}_t)} \\
        &= \frac{\alpha_j(t) \beta_j(t)}{\alpha_E} \times \frac{c_{jm}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jm}^{\text{old}}, \boldsymbol{\Sigma}_{jm}^{\text{old}})}{\sum_{k=1}^M c_{jk}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jk}^{\text{old}}, \boldsymbol{\Sigma}_{jk}^{\text{old}})}
    \end{align*}
}

\wde{M-Step: Observation Probabilities ($b_j(\mathbf{x})$) [GMM Updates]}{
    Use $\gamma_{jm}(t)$ as weights to update parameters for component $m$ of state $j$.
    \begin{itemize}
        \item \textbf{Mixture Weights:}
            \[ \hat{c}_{jm}^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_{jm}(t)}{\sum_{k=1}^{M} \sum_{t=1}^{T} \gamma_{jk}(t)} = \frac{\sum_{t=1}^{T} \gamma_{jm}(t)}{\sum_{t=1}^{T} \gamma_j(t)} \]
        \item \textbf{Means:}
            \[ \hat{\boldsymbol{\mu}}_{jm}^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_{jm}(t) \mathbf{x}_t}{\sum_{t=1}^{T} \gamma_{jm}(t)} \]
        \item \textbf{Covariances:} (Update $\hat{\boldsymbol{\mu}}_{jm}^{\text{new}}$ first)
            \[ \hat{\boldsymbol{\Sigma}}_{jm}^{\text{new}} = \frac{\sum_{t=1}^{T} \gamma_{jm}(t) (\mathbf{x}_t - \hat{\boldsymbol{\mu}}_{jm}^{\text{new}})(\mathbf{x}_t - \hat{\boldsymbol{\mu}}_{jm}^{\text{new}})^T}{\sum_{t=1}^{T} \gamma_{jm}(t)} \]
    \end{itemize}
}

% ==================================================
% == Section 5: Context-Dependent Modelling ==
% ==================================================

\wde{Context-Dependent Phone Models: Motivation}{
    Phonetic context significantly affects acoustic realization (coarticulation). E.g., /n/ in 'ten' vs 'tenth'.
    Using context-independent models (monophones) ignores this, violating the Markov assumption (as the observation probability implicitly depends on more than the current state).
    Context-dependent models aim to capture these variations.
}

\wde{Context-Dependent Units}{
    \begin{itemize}
        \item \textbf{Biphones:} Model depends on either the left or right context. (e.g., `s-ih`, `ih+k`).
        \item \textbf{Triphones:} Model depends on both left and right context (e.g., `s-ih+k`). Notation: `l-x+r`. Triphones are most common.
        \item \textbf{Word-Internal vs. Cross-Word Triphones:} Word-internal only consider context within a word boundary. Cross-word consider context across word boundaries (more accurate but many more potential triphones).
    \end{itemize}
    Using context increases the number of potential models drastically (e.g., $40^3 \approx 64,000$ potential triphones for 40 phones), leading to parameter explosion and data sparsity issues.
}

\wde{Handling Data Sparsity: Smoothing}{
    Due to the inherent dilemma of richer model structures and available 
    training data also known as Data Sparsity, there are various techniques 
    to resort to. 
    \begin{itemize}
        \item \textbf{Back-off}: The idea to is to use lower order models when 
        there is insufficient training data for the higher order ones e.g. back-off from triphone to biphone etc.
        \item \textbf{Interpolation}: Combine all models to have a weighted sum of the model. E.g. with a triphone model:
        \[
        \hat \lambda = \alpha_3 \lambda^{tri} + \alpha_2 \lambda^{bi} + \alpha_1 \lambda^{mono} 
        \]
        Estimate interplation parameters $\alpha$ with deleted interpolation
    \end{itemize}
}

\wde{Handling Data Sparsity: Parameter Sharing}{
    Explicitly share parameters or models across different contexts to pool training data and enable robust estimation for less frequent units. This combats data sparsity inherent in context-dependent modeling. Sharing can occur at different levels:
    \begin{itemize}
        \item \textbf{State Clustering (Tied States):} Groups acoustically similar context-dependent states (e.g., middle states of `l1-x+r1`, `l2-x+r2`) identified via phonetic decision trees based on context features. Tied states share identical output GMM parameters, allowing robust estimation from pooled data. This is the most common and effective method.
        \item \textbf{Tied Mixtures (Sharing Gaussians):} States share a common pool (global 'codebook') of Gaussian components. Each state distribution then learns only its specific mixture weights, significantly reducing parameters compared to fully independent GMMs per state.
        \item \textbf{Generalized Triphones (Sharing Models):} Merges entire HMM models (all states and transitions) for contexts judged acoustically similar (e.g., `(s,f)-iy+l` representing both `s-iy+l` and `f-iy+l`). Reduces the total number of distinct HMM models needed in the system.
    \end{itemize}
}

% ==================================================
% == Section 6: Large Vocabulary Decoding ==
% ==================================================

\wde{Decoding for Large Vocabulary Continuous Speech Recognition (LVCSR)}{
    Finding $W^* = \arg \max_{W} p(X|W) P(W)$ involves searching a huge space.
    \begin{itemize}
        \item \textbf{Connected Words:} Requires transitions between word HMMs.
        \item \textbf{Language Model Integration:} LM scores $P(W)$ (e.g., N-grams) are applied during the search, typically at word boundaries.
        \item \textbf{Complexity Issues:} Large vocabulary $V$, context-dependent models (many states), and higher-order LMs (like trigrams requiring state duplication based on word history) make exact Viterbi search infeasible.
    \end{itemize}
}

\wde{Efficient LVCSR Search Techniques}{
    Approximate search methods are essential:
    \begin{itemize}
        \item \textbf{Beam Search Pruning:} At each time step $t$, discard paths whose score falls too far below the best path score. Keep only paths within a "beam" of the best score.
            \[ V_j(t) > (\min_i V_i(t)) + \beta \quad \text{(Using log probabilities)} \]
        \item \textbf{Tree-Structured Lexicon:} Organize the dictionary (pronunciations) as a tree, merging common phone prefixes (e.g., 'speak', 'spell', 'speech' share initial /s/ /p/). Reduces redundant computations during the search.
        \item \textbf{Language Model Lookahead:} Used with tree lexicons. Pre-calculate the best possible LM score achievable from any node onwards in the tree. Use this estimate during search to adjust path scores and improve pruning decisions by considering future linguistic likelihood earlier.
    \end{itemize}
    Other techniques include multi-pass decoding, A* search, and WFST-based decoding (mentioned in passing).
}