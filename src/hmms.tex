\wde{Objective of ASR}{
    Given a sequence of acoustic feature vectors $X$,
    and $W$ denotes a word sequence, ASR aims to find the most likely word sequence $W^*$
    \begin{equation*}
        W^* = \argmax_{W} P(W | X)
    \end{equation*}
}

\wc{Decomposition of $P(W | X)$}{
    We can decompose $P(W | X)$ with Bayes' theorem:
    \begin{equation*}
        P(W | X) = \frac{P(X | W) P(W)}{P(X)} \propto P(X | W) P(W)
    \end{equation*}
    $P(X | W)$ is the \textbf{Acoustic Model} and $P(W)$ is the \textbf{Language Model}.
}

\wde{Modelling the Acoustic Model with HMMs}{
    Commonly, the left-to-right HMM is used to model the acoustic model. 
    As a word is composed of multiple phonemes, we can 
    model each phoneme with a left-to-right HMM and then concatenate them to form a HMM for the word.
    For the phoneme HMMs, we typically use a left-to-right HMM with 3 states 
    which as a consequence also enforces a minimum phone duration.
    % TODO: maybe add a diagram here
}

\wde{The three fundamental problems of HMMs}{
    \begin{enumerate}
        \item \textbf{Likelihood} - Determine the overall likelihood 
        of an observation sequence $X = x_1, x_2, \ldots, x_T$ given an HMM topology $\mathcal{M}$: \[ P(X|\mathcal{M}) \]
        \item \textbf{Decoding} - Determine the most likely sequence of hidden states $Q = q_1, q_2, \ldots, q_T$
        given an observation sequence $X = x_1, x_2, \ldots, x_T$ and an HMM topology $\mathcal{M}$
        \item \textbf{Training} - Given an observation sequence $X = x_1, x_2, \ldots, x_T$ and an HMM topology $\mathcal{M}$,
        determine the optimal state occupation probabilities.
    \end{enumerate}
}


\wa{Forward Probability [$\alpha_j(t)$]}{
    The \textbf{Likelihood} and \textbf{Training} problem rely on forward probabilities. 
    The forward probability $\alpha_j (t)$ is the probability of observing the observation sequence 
    $x_1, \cdots, x_t$ and being in state $j$ at time $t$.
    This can be computed recursively:
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            \alpha_j (0) &= 1 \quad j = 0 \\
            \alpha_j (0) &= 0 \quad j \neq 0
        \end{align*}
        \item Recursion:
        \begin{equation*}
            \alpha_j (t) = \left( \sum_{i=0}^{J} \alpha_i (t-1) a_{ij} \right) b_j (x_t)
        \end{equation*}
        \item Termination:
        \begin{equation*}
            P(X | \mathcal{M}) = \alpha_{E} = \sum_{i=1}^{J} \alpha_i (T) a_{iE}
        \end{equation*}
    \end{itemize}
}

\wa{Viterbi Algorithm}{
    The \textbf{Decoding} problem is solved by the \textbf{Viterbi Algorithm}.
    Define likelihood of the most probable partial path in state $j$ at time $t$ as $V_j (t)$
    and the Viterbi backpointer as $B_j (t)$ (the most probable state at time $t-1$ that leads to state $j$ at time $t$).
    Then the Viterbi algorithm can be described as:
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            V_0 (0) &= 1\\
            V_j (0) &= 0 \quad j \neq 0\\
            B_j (0) &= 0
        \end{align*}
        \item Recursion:
        \begin{align*}
            V_j (t) &= \max_{i=0}^{J} V_i (t-1) a_{ij} b_j (x_t)\\
            B_j (t) &= \argmax_{i=0}^{J} V_i (t-1) a_{ij}
        \end{align*}
        \item Termination:
        \begin{align*}
            V_E &= \max_{j=1}^{J} V_j (T) a_{jE}\\
            B_E &= \argmax_{j=1}^{J} V_j (T) a_{jE}
        \end{align*}
    \end{itemize}
}

\wa{Training: Hard Assignment with Viterbi Training}{
    If we know the state-time alignment of the training data (i.e. the sequence of states that generated the observation sequence),
    then the transition and observation probabilities can be estimated as follows:
    \begin{align*}
        a_{ij} &= \frac{C(i \to j)}{\sum_{k} C(i \to k)}\\
        \mu_j &= \frac{\sum_{t} z_{jt} x_t}{\sum_{t} z_{jt}}\\
        b_j &= \mathcal{N}(x_t | \mu_j, \Sigma_j) 
    \end{align*}
    where $z_{jt}$ is an indicator variable that is 1 if the $t$-th observation was generated by the $j$-th state.
    $C(i \to j)$ is the number of times the transition from state $i$ to state $j$ is made.
}
\wde{Backward Probability [$\beta_j (t)$]}{
    The backward probability $\beta_j(t) = P(x_{t+1}, \dots, x_T | q_t = j, \mathcal{M})$ is the probability of observing the observation sequence from time $t+1$ to the end, given that the HMM is in state $j$ at time $t$.
    \begin{itemize}
        \item Initialisation:
        \begin{equation*}
            \beta_j (T) = a_{jE}
        \end{equation*}
        \item Recursion:
        \begin{equation*}
            \beta_i (t) = \sum_{j=1}^{J} a_{ij} b_j (x_{t+1}) \beta_j (t+1)
        \end{equation*}
        \item Termination:
        \begin{equation*}
            P(X | \mathcal{M}) = \beta_0 (0) = \sum_{j=1}^{J} a_{0j} b_j (x_1) \beta_j (1) = \alpha_E
        \end{equation*}
    \end{itemize}

}

\wde{State Occupation Probability [$\gamma_j(t)$]}{
$\gamma_j(t)$ is the probability of being in state $j$ at time $t$, given the complete observation sequence $X$ and the model $\mathcal{M}$. It combines forward and backward probabilities.
\begin{align*}
    \gamma_j (t) &= P(q_t = j | X, \mathcal{M})\\
    &= \frac{P(q_t = j, X_{1:T} | \mathcal{M})}{P(X|M)}\\
    &= \frac{P(q_t = j, X_{1:t}, X_{t+1:T} | \mathcal{M})}{\alpha_E} \\ % & \text{(Sub. and Exp.)} \\
    &= \frac{P(X_{t+1:T} | q_t = j, X_{1:t}, \mathcal{M}) \times P(q_t = j, X_{1:t} | \mathcal{M})}{\alpha_E} \\ %& \text{(Chain rule)}\\
    &= \frac{P(X_{t+1:T} | q_t = j, \mathcal{M}) \times P(q_t = j, X_{1:t} | \mathcal{M})}{\alpha_E} \\ %& \text{(Markov)}\\
    &= \frac{\beta_j (t) \alpha_j (t)}{\alpha_E} \\%& \text{(Sub.)}
\end{align*}
This represents the expected proportion of time spent in state $j$ at time $t$. Note that $\sum_{j=1}^J \gamma_j(t) = 1$ for any $t$.
}

\wde{Transition Occupation Probability ($\xi_{ij}(t)$)}{
    $\xi_{ij}(t)$ is the probability of being in state $i$ at time $t$ and transitioning to state $j$ at time $t+1$, given the complete observation sequence $X$ and the model $\mathcal{M}$.
    % \begin{enumerate}
    %     \item Starting from the beginning of the sequence, generate observations $X_{1:t}$ and arrive in state $q_t = i$ (forward probability)
    %     \item Transitioning from $q_t = i$ to state $q_{t+1} = j$ (after Markov assumption, this is the same as $a_{ij}$)
    %     \item From $q_{t+1} = j$ generate observation $x_{t+1}$ (after Markov assumption, this is the same as $b_j (x_{t+1})$)
    %     \item Generating the remaining observations $x_{t+2:T}$ (backward probability)
    % \end{enumerate}

    % This leads to the following equation:
    \begin{align*}
        \xi_{ij} (t) &= P(q_t = i, q_{t+1} = j | X, \mathcal{M})\\
        &= \frac{P(q_t = i, q_{t+1} = j, X_{1:T} | \mathcal{M})}{P(X | \mathcal{M})}\\
        &= \frac{P(X_{1:t}, q_t = i, q_{t+1} = j, X_{t+1:T} | \mathcal{M})}{\alpha_E}\\
        &= \frac{\alpha_i (t) a_{ij} b_j (x_{t+1}) \beta_j (t+1)}{\alpha_E}
    \end{align*}
}



% \wde{Modelling the transition probability}{
%     With the transition occupation probability, we can estimate the transition probability as follows:
%     \begin{align*}
%         \hat a_{ij} &= \frac{\sum_{t=1}^{T-1} \xi_{ij} (t)}{\sum_{k=1}^{J} \sum_{t=1}^{T-1} \xi_{ik} (t)}
%     \end{align*}
%     This can be extended to corpus of $R$ utterances by summing (and normalizing) over all utterances:
%     \begin{align*}
%         \hat a_{ij} &= \frac{\sum_{r=1}^{R} \sum_{t=1}^{T} \xi_{i,j}^{r} (t)}{\sum_{r=1}^{R} \sum_{k=1}^{J} \sum_{t=1}^{T} \xi_{ik}^{r} (t)}
%     \end{align*}
% }

% \wde{Modelling the observation probability I}{
%     When we model the observation probability with a multivariate 
%     Gaussian distribution we can use the occuption probabilities $\gamma_j$ 
%     to estimate $\boldsymbol \mu_j$ and $\boldsymbol \Sigma_j$ as follows:
%     \[
%      \boldsymbol \mu_j = \frac{\sum_{t=1}^{T} \gamma_j (t) \boldsymbol x_t}{\sum_{t=1}^{T} \gamma_j (t)}, \quad
%      \boldsymbol \Sigma_j = \frac{\sum_{t=1}^{T} \gamma_j (t) (\boldsymbol x_t - \boldsymbol \mu_j)(\boldsymbol x_t - \boldsymbol \mu_j)^T}{\sum_{t=1}^{T} \gamma_j (t)}
%      \]
%      Then assuming that the acoustic feature emitted at each state $j$ is Gaussian,
%      then, $b_j (\boldsymbol x)$ is:
%      \[
%         b_j (\boldsymbol x) = \mathcal{N}(\boldsymbol x; \boldsymbol \mu_j, \boldsymbol \Sigma_j) 
%      \]
% }

% \wde{Modelling the observation probability II}{
%     The assumption of a Gaussian Distribution at each state 
%     is too \textbf{strong}, and in practice acoustic feature vectors (MFCCs etc.) 
%     associated with a state may be strongly non-Gaussian. As such, 
%     M-component Gaussian Mixture Models (GMMs) is a more appropriate 
%     density function to model $b_j$:
%     \[
%         b_j(\boldsymbol x) = p(\boldsymbol x | q = j) = \sum_{m=1}^{M} c_{jm} \mathcal{N}(\boldsymbol x ; \boldsymbol \mu_{jm}, \boldsymbol \Sigma_{jm})
%     \]
%      $\boldsymbol \mu_j$ and $\boldsymbol \Sigma_j$ requires extending the concept of 
%      state occupation probabilities to GMMs which we define as $\gamma_{jm} (t)$ (component state occupation) which 
%      is the probability of occupying mixture component $m$ of state $j$ at time $t$.
% }

\wa{EM Algorithm (Baum-Welch) for HMM Training}{
    The Baum-Welch algorithm is an instance of the Expectation-Maximization (EM) algorithm used to find the maximum likelihood estimate of the parameters ($\lambda = \{A, B, \pi\}$) of an HMM when the state sequence is unknown. It iteratively performs two steps:
    \begin{itemize}
        \item \textbf{E-step (Expectation):} Using the current parameter estimates $\lambda^{\text{old}}$, compute the expected values needed for the M-step. This involves:
            \begin{itemize}
                \item Computing forward probabilities $\alpha_j(t)$ $\forall j, t$.
                \item Computing backward probabilities $\beta_j(t)$ $\forall j, t$.
                \item Computing state occupation probabilities $\gamma_j(t)$ $\forall j, t$
                \item Computing transition occupation probabilities $\xi_{ij}(t)$ $\forall i,j,t$
                \item If using GMMs for $b_j$, compute component-state occupation probabilities $\gamma_{jm}(t)$ (see below).
            \end{itemize}
        \item \textbf{M-step (Maximization):} Update the parameters $\lambda^{\text{new}}$ to maximize the expected complete-data log-likelihood, using the expectations computed in the E-step. See specific update formulas below.
        \item Repeat E-step and M-step until the parameters or the likelihood $P(X|\lambda)$ converge.
    \end{itemize}
}

% --- M-Step: Parameter Updates ---

\wde{M-Step: Transition Probabilities ($a_{ij}$)}{
    The transition probability $a_{ij}$ is updated as the expected number of transitions from state $i$ to $j$, divided by the expected total number of transitions *out* of state $i$.
    \begin{align*}
        \hat a_{ij}^{\text{new}} &= \frac{\text{Expected transitions } i \to j}{\text{Expected transitions out of } i} \\
        &= \frac{\sum_{t=1}^{T-1} \xi_{ij}(t)}{\sum_{k=1}^{J} \sum_{t=1}^{T-1} \xi_{ik}(t)} = \frac{\sum_{t=1}^{T-1} \xi_{ij}(t)}{\sum_{t=1}^{T-1} \gamma_i(t)}
    \end{align*}
    For a corpus of $R$ utterances, sum the numerators and denominators over all utterances $r=1, \dots, R$.
    % Optional: Add corpus formula if needed
    % \begin{align*}
    %     \hat a_{ij}^{\text{new}} &= \frac{\sum_{r=1}^{R} \sum_{t=1}^{T_r-1} \xi_{ij}^{r}(t)}{\sum_{r=1}^{R} \sum_{t=1}^{T_r-1} \gamma_i^{r}(t)}
    % \end{align*}
}

\wde{M-Step: Observation Probabilities ($b_j(\mathbf{x})$) [Single Gaussian]}{
    If the observation probability for state $j$ is modeled by a single multivariate Gaussian $b_j(\mathbf{x}) = \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j)$, the parameters are updated using weighted maximum likelihood, where the weights are the state occupation probabilities $\gamma_j(t)$.
    \begin{align*}
        \hat{\boldsymbol{\mu}}_j^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_j(t) \mathbf{x}_t}{\sum_{t=1}^{T} \gamma_j(t)} \\
        \hat{\boldsymbol{\Sigma}}_j^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_j(t) (\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})(\mathbf{x}_t - \hat{\boldsymbol{\mu}}_j^{\text{new}})^T}{\sum_{t=1}^{T} \gamma_j(t)}
    \end{align*}
}

\wde{M-Step: Observation Probabilities ($b_j(\mathbf{x})$) [GMM]}{
    If $b_j(\mathbf{x})$ is an M-component GMM, $b_j(\mathbf{x}) = \sum_{m=1}^{M} c_{jm} \mathcal{N}(\mathbf{x}; \boldsymbol{\mu}_{jm}, \boldsymbol{\Sigma}_{jm})$, we need the component-state occupation probability $\gamma_{jm}(t)$.
    \begin{itemize}
        \item \textbf{Component-State Occupation Probability ($\gamma_{jm}(t)$):} This is the probability of being in state $j$ and using component $m$ at time $t$, given $X$. It's calculated in the E-step after $\gamma_j(t)$:
            \begin{align*}
                \gamma_{jm}(t) &= P(q_t = j, c_t = m | X, \mathcal{M}^{\text{old}}) \\
                &= \gamma_j(t) \times \frac{c_{jm}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jm}^{\text{old}}, \boldsymbol{\Sigma}_{jm}^{\text{old}})}{b_j^{\text{old}}(\mathbf{x}_t)} \\
                &= \frac{\alpha_j(t) \beta_j(t)}{\alpha_E} \times \frac{c_{jm}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jm}^{\text{old}}, \boldsymbol{\Sigma}_{jm}^{\text{old}})}{\sum_{k=1}^M c_{jk}^{\text{old}} \mathcal{N}(\mathbf{x}_t; \boldsymbol{\mu}_{jk}^{\text{old}}, \boldsymbol{\Sigma}_{jk}^{\text{old}})}
            \end{align*}
        \item \textbf{GMM Parameter Updates (M-step):} Use $\gamma_{jm}(t)$ as weights for component $m$ of state $j$.
            \begin{align*}
                \hat{c}_{jm}^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_{jm}(t)}{\sum_{k=1}^{M} \sum_{t=1}^{T} \gamma_{jk}(t)} = \frac{\sum_{t=1}^{T} \gamma_{jm}(t)}{\sum_{t=1}^{T} \gamma_j(t)} \\
                \hat{\boldsymbol{\mu}}_{jm}^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_{jm}(t) \mathbf{x}_t}{\sum_{t=1}^{T} \gamma_{jm}(t)} \\
                \hat{\boldsymbol{\Sigma}}_{jm}^{\text{new}} &= \frac{\sum_{t=1}^{T} \gamma_{jm}(t) (\mathbf{x}_t - \hat{\boldsymbol{\mu}}_{jm}^{\text{new}})(\mathbf{x}_t - \hat{\boldsymbol{\mu}}_{jm}^{\text{new}})^T}{\sum_{t=1}^{T} \gamma_{jm}(t)}
            \end{align*}
    \end{itemize}
}