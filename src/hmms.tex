\wde{Objective of ASR}{
    Given a sequence of acoustic feature vectors $X$,
    and $W$ denotes a word sequence, ASR aims to find the most likely word sequence $W^*$
    \begin{equation*}
        W^* = \argmax_{W} P(W | X)
    \end{equation*}
}

\wc{Decomposition of $P(W | X)$}{
    We can decompose $P(W | X)$ with Bayes' theorem:
    \begin{equation*}
        P(W | X) = \frac{P(X | W) P(W)}{P(X)} \propto P(X | W) P(W)
    \end{equation*}
    $P(X | W)$ is the \textbf{Acoustic Model} and $P(W)$ is the \textbf{Language Model}.
}

\wde{Modelling the Acoustic Model with HMMs}{
    Commonly, the left-to-right HMM is used to model the acoustic model. 
    As a word is composed of multiple phonemes, we can 
    model each phoneme with a left-to-right HMM and then concatenate them to form a HMM for the word.
    For the phoneme HMMs, we typically use a left-to-right HMM with 3 states 
    which as a consequence also enforces a minimum phone duration.
    % TODO: maybe add a diagram here
}

\wde{The three fundamental problems of HMMs}{
    \begin{enumerate}
        \item \textbf{Likelihood} - Determine the overall likelihood 
        of an observation sequence $X = x_1, x_2, \ldots, x_T$ given an HMM topology $\mathcal{M}$
        \item \textbf{Decoding} - Determine the most likely sequence of hidden states $Q = q_1, q_2, \ldots, q_T$
        given an observation sequence $X = x_1, x_2, \ldots, x_T$ and an HMM topology $\mathcal{M}$
        \item \textbf{Training} - Given an observation sequence $X = x_1, x_2, \ldots, x_T$ and an HMM topology $\mathcal{M}$,
        determine the optimal state occupation probabilities.
    \end{enumerate}
}

\wa{Forward Algorithm}{
    The \textbf{Likelihood} problem is solved by the \textbf{Forward Algorithm}.
    The forward probability $\alpha_j (t)$ is the probability of observing the observation sequence 
    $x_1, \cdots, x_t$ and being in state $j$ at time $t$.
    \begin{equation*}
        \alpha_j (t) = P(x_1, \cdots, x_t, q_t = j | \mathcal{M})
    \end{equation*}
    This can be computed recursively:
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            \alpha_j (0) &= 1 \quad j = 0 \\
            \alpha_j (0) &= 0 \quad j \neq 0
        \end{align*}
        \item Recursion:
        \begin{equation*}
            \alpha_j (t) = \left( \sum_{i=0}^{J} \alpha_i (t-1) a_{ij} \right) b_j (x_t)
        \end{equation*}
        \item Termination:
        \begin{equation*}
            P(X | \mathcal{M}) = \alpha_{E} = \sum_{i=1}^{J} \alpha_i (T) a_{iE}
        \end{equation*}
    \end{itemize}
}

\wa{Viterbi Algorithm}{
    The \textbf{Decoding} problem is solved by the \textbf{Viterbi Algorithm}.
    Define likelihood of the most probable partial path in state $j$ at time $t$ as $V_j (t)$
    and the Viterbi backpointer as $B_j (t)$ (the most probable state at time $t-1$ that leads to state $j$ at time $t$).
    Then the Viterbi algorithm can be described as:
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            V_0 (0) &= 1\\
            V_j (0) &= 0 \quad j \neq 0\\
            B_j (0) &= 0
        \end{align*}
        \item Recursion:
        \begin{align*}
            V_j (t) &= \max_{i=0}^{J} V_i (t-1) a_{ij} b_j (x_t)\\
            B_j (t) &= \argmax_{i=0}^{J} V_i (t-1) a_{ij}
        \end{align*}
        \item Termination:
        \begin{align*}
            V_E &= \max_{j=1}^{J} V_j (T) a_{jE}\\
            B_E &= \argmax_{j=1}^{J} V_j (T) a_{jE}
        \end{align*}
    \end{itemize}
}

\wa{Training: Hard Assignment with Viterbi Training}{
    If we know the state-time alignment of the training data (i.e. the sequence of states that generated the observation sequence),
    then the transition and observation probabilities can be estimated as follows:
    \begin{align*}
        a_{ij} &= \frac{C(i \to j)}{\sum_{k} C(i \to k)}\\
        \mu_j &= \frac{\sum_{t} z_{jt} x_t}{\sum_{t} z_{jt}}\\
        b_j &= \mathcal{N}(x_t | \mu_j, \Sigma_j) 
    \end{align*}
    where $z_{jt}$ is an indicator variable that is 1 if the $t$-th observation was generated by the $j$-th state.
    $C(i \to j)$ is the number of times the transition from state $i$ to state $j$ is made.
}

\wde{Backward Algorithm}{
    Similar to the forward probability, we can define the backward probability $\beta_j (t)$ as the probability of observing the observation sequence 
    $x_{t+1}, \cdots, x_T$ given that the current state is $j$ at time $t$.
    \begin{equation*}
        \beta_j (t) = P(x_{t+1}, \cdots, x_T | q_t = j, \mathcal{M})
    \end{equation*}
    This can be computed recursively:
    \begin{itemize}
        \item Initialisation:
        \begin{equation*}
            \beta_j (T) = a_{jE}
        \end{equation*}
        \item Recursion:
        \begin{equation*}
            \beta_i (t) = \sum_{j=1}^{J} a_{ij} b_j (x_{t+1}) \beta_j (t+1)
        \end{equation*}
        \item Termination:
        \begin{equation*}
            P(X | \mathcal{M}) = \beta_0 (0) = \sum_{j=1}^{J} a_{0j} b_j (x_1) \beta_j (1) = \alpha_E
        \end{equation*}
    \end{itemize}

}

\wde{State Occupation Probabilities}{
$\lambda_j (t)$ is the state occupation probability which is 
the probability of occupying state $j$ at time $t$ given the sequence of observations.
It is expressed in terms of the \textbf{Forward} and \textbf{Backward} probabilities as follows:
\begin{equation*}
    x
\end{equation*}
}

\wa{Training: Soft Assignment with Baum-Welch Algorithm}{
    The issue is that 
}