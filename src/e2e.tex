\wde{End-to-end Systems}{
    End-to-end systems are systems which learn to directly map from 
    an input sequence $X$ to an output sequence $Y$. Where HMMs 
    are comprised of multiple components (acoustic model, language model, lexicon) 
    that are optimised individually, fully end-to-end system optimise a 
    \textbf{single} objective function, drastically simplifying the pipeline 
    in developing ASR systems. This approach is commonly used in state-of-the-art 
    ASR systems now.
}

\wde{Connectionist Temporal Classification (CTC)}{
    An alignment-free approach for sequence-to-sequence tasks where input length $T$ is typically much larger than output length $M$ (e.g., speech frames to characters/phones). Uses a neural network (RNN/Transformer) to output frame-level probabilities $P_t(c_t|X)$ over target labels plus a special \textbf{blank token} ($\epsilon$).
    \begin{itemize}
        \item \textbf{Alignment Mapping ($\mathcal{B}$):} A many-to-one function maps frame-level output sequences $C$ (length $T$) to target sequences $Y$ (length $M \le T$) by first merging consecutive identical non-blank labels, then removing all $\epsilon$ blanks. E.g., $\mathcal{B}(a\epsilon\epsilon b b \epsilon c) = abc$.
        \item \textbf{Properties:} Alignments are monotonic. Allows variable timing and avoids needing explicit segmentation.
        \item \textbf{Conditional Independence Assumption:} The probability of a full alignment $C$ is calculated assuming frame outputs are independent given the input $X$: $P(C|X) = \prod_{t=1}^T P_t(c_t|X)$. The underlying NN (e.g., BiLSTM) aims to make $P_t(c_t|X)$ depend on the full $X$.
        \item \textbf{Handling Repeats:} To output repeated characters (e.g., "hello"), the alignment $C$ must include $\epsilon$ between them (e.g., $h e l \epsilon l o$).
    \end{itemize}
}

\wde{CTC Loss Function}{
    Trains the NN to maximize the total probability of the correct target sequence $Y$, by summing over all possible frame-level alignments $C$ that map to $Y$ via the compression function $\mathcal{B}$.
    Let $A(Y) = \{C | \mathcal{B}(C) = Y\}$ be the set of valid alignments for $Y$.
    \[ P(Y|X) = \sum_{C \in A(Y)} P(C | X) = \sum_{C \in A(Y)} \prod_{t=1}^T P_t(c_t|X) \]
    The CTC loss is the negative log likelihood: $\mathcal{L}_{CTC} = - \log P(Y|X)$.

    This sum over alignments $P(Y|X)$ is computed efficiently using dynamic programming via the \textbf{CTC Forward Algorithm}.
    First, define the expanded symbol sequence including blanks $Z = (\epsilon, s_1, \epsilon, s_2, \dots, \epsilon, s_M, \epsilon)$, with length $J = 2M + 1$. Let $z_j$ be the $j$-th symbol in $Z$.
    The forward variable $\alpha_j(t)$ represents the total probability of all paths ending in symbol $z_j$ at time $t$.
    \begin{itemize}
        \item Initialisation:
        \begin{align*}
            \alpha_{1}(1) &= P_1(z_1=\epsilon | X) \\ % Prob of blank at t=1
            \alpha_{2}(1) &= P_1(z_2=s_1 | X) \\ % Prob of first char at t=1
            \alpha_{j}(1) &= 0 \quad \text{for } j > 2
        \end{align*}
        % Note: A slightly different initialization might be used depending on source,
        % e.g., alpha_i(0)=1 for i=1, 0 otherwise, then start recursion at t=1.
        % The one below matches the slide recursion structure better.
        % Let's stick to the slide's initialization for consistency:
        % \item Initialisation:
        % \begin{align*}
        %     \alpha_{1} (0) &= 1 \\ % Start in blank state at t=0
        %     \alpha_{j} (0) &= 0 \quad \text{for } j > 1
        % \end{align*}
        \item Recursion ($t=2, \dots, T$; $j=1, \dots, J$):
            \begin{itemize}
                \item If $z_j = \epsilon$ or $z_j = z_{j-2}$ (current is blank or same as state before previous blank):
                \[ \alpha_{j}(t) = (\alpha_{j-1}(t-1) + \alpha_{j}(t-1)) P_t(z_j | X) \]
                (Can come from previous symbol $z_{j-1}$ or stay in $z_j$)
                \item Otherwise ($z_j$ is a non-blank, different from $z_{j-2}$):
                \[ \alpha_{j}(t) = (\alpha_{j-2}(t-1) + \alpha_{j-1}(t-1) + \alpha_{j}(t-1)) P_t(z_j | X) \]
                (Can come from $z_{j-2}$, $z_{j-1}$, or stay in $z_j$)
            \end{itemize}
        \item Termination: The total probability $P(Y|X)$ is the sum of probabilities ending in the last blank state ($z_J$) or the last actual symbol state ($z_{J-1}$) at the final time step $T$.
        \[ P(Y|X) = \alpha_{J-1}(T) + \alpha_{J}(T) \]
    \end{itemize}
    (Note: Gradient calculation for training involves a similar backward pass).
}

\wde{CTC Decoding/Inference}{
    Goal: Find the most likely output sequence $Y^* = \arg\max_Y P(Y|X)$.
    \begin{itemize}
        \item \textbf{Simple Greedy/Viterbi is Suboptimal:} Finding the single best frame-level alignment $C^* = \arg\max_C \prod_t P_t(c_t|X)$ and compressing it ($\mathcal{B}(C^*)$) is easy but often incorrect. As noted in the lectures (Slide 30), this $C^*$ might not correspond to the $Y$ with the highest total probability $P(Y|X)$, because $P(Y|X)$ requires summing over *all* valid alignments $C \in A(Y)$.
        \item \textbf{Beam Search Decoding:} Standard approach to approximate $\arg\max_Y P(Y|X)$. Maintain a beam of candidate output prefixes $Y_{prefix}$. At each time step $t$:
            \begin{itemize}
                \item Extend existing prefixes with new characters (non-blank or blank) using $P_t(c_t|X)$.
                \item \textbf{Merge Probabilities:} Crucially, add probabilities for hypotheses that result in the \textit{same output prefix} after applying $\mathcal{B}$.
                \item Prune hypotheses outside the beam width.
            \end{itemize}
    \end{itemize}
}

\wde{Incorporating a Language Model with CTC}{
    CTC's conditional independence assumption means an external LM is usually needed. The common approach integrates the LM during decoding (often called \textbf{Shallow Fusion}):
    \begin{itemize}
        \item \textbf{Combined Score:} During beam search, the score for a word sequence $W$ (corresponding to sub-word sequence $Y$) is calculated by interpolating the CTC log probability and the LM log probability:
            \begin{multline*}
             \hat{W} = \argmax_{W} \bigl( \log P_{CTC}(Y|X) 
                    + \lambda \log P_{LM}(W) + \eta L(W) \bigr)
            \end{multline*}
            (Where $L(W)$ is often word count for length bonus).
        \item \textbf{Parameters:} $\lambda$ (LM weight) and $\eta$ (word insertion bonus) are tuned on a development set.
        \item \textbf{Application:} The LM score $\log P_{LM}(W)$ is typically added when a complete word is hypothesized or incrementally as sub-word units are added during the beam search.
        \item \textbf{Alternative:} WFSTs can also be used by creating a CTC topology transducer (T) and composing $T \circ L \circ G$.
    \end{itemize}
}

\wde{RNN Transducer (RNN-T)}{
    \begin{itemize}
        \item \textbf{Motivation:} CTC assumes output labels are independent given the input $X$, limiting its ability to learn output dependencies (like an LM).
        \item \textbf{Architecture:} Uses three components:
            \begin{itemize}
                \item \textbf{Encoder (AM):} Maps acoustic input $X \to h^{enc}$ (like CTC).
                \item \textbf{Prediction Net (Implicit LM):} RNN using previous output label $y_{u-1}$ to predict next label context $p_u$.
                \item \textbf{Joint Net:} Combines $h^{enc}_t$ and $p_u$ to predict the probability $P(k|t,u)$ of the next output label $k$ (incl. blank $\epsilon$).
            \end{itemize}
        \item \textbf{Key Difference:} Output probability depends on both acoustic context ($t$) and previous output label context ($u$), allowing it to learn local output dependencies.
    \end{itemize}
}
\wde{Attention Encoder-Decoder (AED) Models}{
    Sequence-to-sequence models (e.g., Google's Listen, Attend, Spell) that address limitations of CTC/RNN-T's strict monotonic alignment.
    \begin{itemize}
        \item \textbf{Motivation:} CTC and RNN-T enforce a frame-by-frame or near-monotonic alignment between input and output, which can be restrictive for tasks where output timing isn't strictly tied to input timing (e.g., non-speech seq2seq tasks). AED allows flexible, non-monotonic alignments by letting the model decide which input parts to focus on for each output.
        \item \textbf{Architecture:} Encoder (RNN/Transformer) processes input $X \to h^{enc}$, capturing the full utterance context. Decoder (RNN) generates output $Y = (y_1..y_U)$ step-by-step.
        \item \textbf{Attention Mechanism (Key Innovation):} At each decoder step $u$, attention computes weights $\alpha_{ut}$ over all encoder states $h^{enc}_t$ based on the current decoder state $h^{dec}_u$. A context vector $c_u = \sum_t \alpha_{ut} h^{enc}_t$ summarizes relevant input parts for predicting $y_u$. $P(y_u | X, y_{<u})$ depends on $h^{dec}_u$ and $c_u$.
        \item \textbf{Pros/Cons:} Offers flexible alignment and implicitly integrates an LM in the decoder. Can be less robust for strict monotonic tasks like ASR without regularization (e.g., CTC loss). Often slower to decode due to autoregressive nature.
        \item \textbf{Hybrid CTC/Attention:} Jointly training with CTC loss on the encoder acts as a regularizer, encouraging more monotonic alignments typical in speech, often improving performance and robustness.
    \end{itemize}
}

\wde{Self-Supervised Learning (SSL) for Speech}{
    Leverages vast amounts of \textbf{unlabeled} audio data to learn powerful speech representations via \textbf{pretext tasks}, reducing reliance on expensive labeled data.
    \begin{itemize}
        \item \textbf{Motivation:} Transcribed speech data is scarce and costly to obtain, limiting supervised training. SSL pre-trains models on unlabeled audio (abundant) to learn general-purpose representations capturing phonetics, prosody, speaker traits, and even some linguistic structure, which can then be adapted to specific tasks with minimal labeled data.
        \item \textbf{Pretext Tasks (How it Learns):}
            \begin{itemize}
                \item \textbf{Contrastive (e.g., CPC, wav2vec 2.0):} Train model to distinguish correct future or masked representations from negative samples (distractors). For wav2vec 2.0, a CNN+Transformer processes masked input, predicting quantized targets for masked frames via contrastive loss.
                \item \textbf{Predictive (e.g., HuBERT):} Train model to predict discrete target units (e.g., k-means clusters of MFCCs or prior model features) for masked input frames using cross-entropy loss. Simpler than contrastive, focuses on pseudo-phonetic content.
            \end{itemize}
        \item \textbf{Usage (Why it's Powerful):}
            \begin{itemize}
                \item \textbf{Fine-tuning:} Pre-train on unlabeled data (e.g., 1000s of hours), then fine-tune the model (or add a small task-specific head like CTC) on a small labeled dataset for ASR or other tasks. Achieves state-of-the-art results with less labeled data than supervised approaches.
                \item \textbf{Feature Extraction:} Use embeddings from specific layers of pre-trained SSL models (e.g., HuBERT) as rich input features for downstream models. Different layers often capture different information (low layers: acoustics, high layers: linguistic).
                \item \textbf{Probing:} Analyze what SSL models learn by training simple classifiers on their features to detect phones, words, or semantics, revealing hierarchical knowledge.
            \end{itemize}
        \item \textbf{Impact:} SSL models like wav2vec 2.0 and HuBERT are essential for modern ASR, enabling high performance in low-resource settings and serving as pre-trained system components
    \end{itemize}
}

\wde{ASR with Large Language Models (LLMs)}{
    Leverages powerful pre-trained text-based LLMs (e.g., BERT, GPT) for speech tasks, capitalizing on their vast linguistic knowledge.
    \begin{itemize}
        \item \textbf{Motivation:} E2E models (CTC, RNN-T, AED) integrate AM and LM implicitly but may not match the linguistic modeling power of text-only LLMs trained on massive datasets. LLMs can enhance ASR accuracy, especially for rare words or complex language, and enable multi-task capabilities.
        \item \textbf{Decoder-Only ASR:} Treat ASR as conditional language modeling. Use a fixed pre-trained acoustic encoder (e.g., HuBERT) to extract features $h^{enc}$. Project/compress $h^{enc}$ into a shorter sequence $H$ compatible with LLM text embeddings (via discrete units from SSL, CTC compression, or downsampling). Prepend $H$ to the input of a text LLM decoder, which generates transcript $Y$ autoregressively, conditioned on $H$.
        \item \textbf{Instruction Tuning \& Zero-Shot ASR:} Use prompts (e.g., "Transcribe this audio:") with acoustic embedding $H$ to guide an instruction-tuned LLM for zero-shot ASR or related tasks (translation, summarization) without task-specific fine-tuning.
        \item \textbf{ASR Error Correction:} Use an LLM to post-process and correct transcripts from a primary ASR system with prompts like "Correct this transcription:". Improves accuracy, especially for domain-specific terms if context (e.g., glossaries) is provided.
    \end{itemize}
}

\wde{Choosing the Right ASR Approach}{
    Selecting between HMM-based, End-to-End (E2E), and LLM-enhanced systems depends on task requirements, data availability, and computational resources:
    \begin{itemize}
        \item \textbf{HMM-based (HMM/GMM, HMM/DNN):} Use for well-understood, controlled domains with sufficient labeled data and need for interpretability (e.g., modular AM/LM). Best for legacy systems or when fine-grained control over components (e.g., adaptation via MLLR) is needed. Less effective with limited data or complex language.
        \item \textbf{End-to-End (CTC, RNN-T, AED):} Preferred for modern ASR with large labeled datasets. Simplifies pipeline (single model), excels in diverse, noisy conditions, and integrates AM/LM implicitly. CTC/RNN-T better for streaming (low latency); AED for tasks needing flexible alignment. Requires significant data and compute.
        \item \textbf{SSL + Fine-tuning (e.g., wav2vec 2.0, HuBERT):} Ideal for low-resource scenarios. Pre-train on unlabeled audio, fine-tune on small labeled data. Achieves state-of-the-art with less supervision. Use when labeled data is scarce or for transfer learning to new languages/domains.
        \item \textbf{LLM-enhanced ASR:} Use for highest accuracy, especially with rare words, complex language, or multi-task needs (transcription + translation). Best when paired with strong acoustic encoders (SSL-based) and for error correction in specialized domains (e.g., medical, legal) with context. High compute cost, often non-streaming.
    \end{itemize}
}

% ==================================================
% == Section 11: Multilingual & Low-Resource ASR ==
% ==================================================

\wde{Low-Resource Challenges}{
    Most languages lack labeled audio, text, lexica, expertise. Issues:
    \begin{itemize}
        \item \textbf{Data Scarcity:} Limits robust training.
        \item \textbf{Morphology:} Rich languages have huge vocabularies, high OOV rates.
        \item \textbf{Code-Switching:} Mixing languages in utterances.
        \item \textbf{Lexicon:} Hard to create pronunciation dictionaries.
    \end{itemize}
}

\wde{Solutions for Low-Resource ASR}{
    \begin{itemize}
        \item \textbf{Subword Units:} Use morphs, BPE, WordPiece to reduce vocabulary, handle OOV (Tools: Morfessor, SentencePiece).
        \item \textbf{Multilingual Models:} Share layers/features across languages (e.g., Block Softmax, bottleneck features).
        \item \textbf{Cross-lingual SSL:} Pre-train on multilingual unlabeled audio (e.g., XLSR, Whisper), fine-tune on small target data.
        \item \textbf{Semi-Supervised:} Use seed model to transcribe unlabeled data, retrain with care (confidence scores, MMI).
        \item \textbf{Grapheme/Subword E2E:} Output characters/subwords directly, avoiding lexica (CTC, RNN-T, AED).
        \item \textbf{Zero-Resource Goal:} ASR with no labeled data via unsupervised unit discovery (wav2vec-U) or cross-lingual mapping.
    \end{itemize}
}